# Copyright 2020 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Script for running CoinDICE with neural network function approximators.

The default parameters here should reproduce the published reacher results. 
Make sure to generate the reacher dataset prior to running this script (see
`scripts/create_dataset.py`). 
Furthermore, the user will need to feed in an appropriate `divergence_limit`, which should be set to a desired chi2 percentile
divided by the size of the offline dataset (see paper for details). For example,
if a 90% confidence interval is desired and the offline dataset is 25
trajectories of length 100, then the divergence_limit should be 2.7055 / 2500.
"""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as np
import os
import tensorflow.compat.v2 as tf
tf.compat.v1.enable_v2_behavior()
import tensorflow_probability as tfp
import pickle

from tf_agents.environments import gym_wrapper
from tf_agents.environments import tf_py_environment



from coinDice import common as common_utils
from coinDice import estimator as estimator_lib
from coinDice.dataset import Dataset, EnvStep, StepType
from coinDice.env_policies import get_target_policy

from coinDice.value_network import ValueNetwork
from coinDice.neural_coin_dice import  NeuralCoinDice

##########################################################################################

flags.DEFINE_string('load_dir', None,
                    'Directory to load dataset from.')
flags.DEFINE_string('save_dir', None,
                    'Directory to save results to.')

""" TODO
I should try its original dataset, to see what is inside
1. dataset_spec: The spec of the dataset that will be given.
2. dir
4. output_dim=2 * 2 * n_intervals
"""


##########################################################################################
def main(gamma = .95, seed = 42
        , nu_learning_rate = 0.01, zeta_learning_rate = 0.001, weight_learning_rate = 0.001
        , nu_regularizer = 0, zeta_regularizer = 0, batch_size = 4096, num_steps = 200000
        , tabular_obs = False
        , algae_alpha = 0.01, alpha = 0, f_exponent = 1.5, primal_form = True, divergence_limit = 0.001):
  ################################################################################################################################################
  """ env_name """
  target_policy = get_target_policy(load_dir, env_name, tabular_obs)

  #################################################################################################################################################
  dataset = Dataset.load(directory)
  all_steps = dataset.get_all_steps()
  max_reward = tf.reduce_max(all_steps.reward)
  min_reward = tf.reduce_min(all_steps.reward)
  print('num loaded steps', dataset.num_steps)
  print('num loaded total steps', dataset.num_total_steps)
  print('num loaded episodes', dataset.num_episodes)
  print('num loaded total episodes', dataset.num_total_episodes)
  print('min reward', min_reward, 'max reward', max_reward)
  ################################################################################################################################################
  estimate = estimator_lib.get_fullbatch_average(dataset, gamma=gamma)
  print('data per step avg', estimate)

  train_hparam_str = ('nlr{NU_LR}_zlr{Z_LR}_batch{BATCH_SIZE}_'
                      'gam{GAMMA}_nreg{NU_REG}_zreg{Z_REG}_algae{ALGAE_ALPHA}_'
                      'prim{PRIMAL}_div{DIV}').format(
                          NU_LR=nu_learning_rate,
                          Z_LR=zeta_learning_rate,
                          BATCH_SIZE=batch_size,
                          GAMMA=gamma,
                          NU_REG=nu_regularizer,
                          Z_REG=zeta_regularizer,
                          ALGAE_ALPHA=algae_alpha,
                          PRIMAL=primal_form,
                          DIV=divergence_limit)
  
  ################################################################################################################################################
  activation_fn = tf.nn.relu
  kernel_initializer = tf.keras.initializers.TruncatedNormal(
      stddev=0.5, seed=1)
  hidden_dims = (64,)
  n_intervals = 1
  
  nu_network = ValueNetwork((dataset.spec.observation, dataset.spec.action),
                            fc_layer_params=hidden_dims,
                            activation_fn=activation_fn,
                            kernel_initializer=kernel_initializer,
                            last_kernel_initializer=None,
                            output_dim=2 * 2 * n_intervals)
  zeta_network = ValueNetwork((dataset.spec.observation, dataset.spec.action),
                              fc_layer_params=hidden_dims,
                              activation_fn=activation_fn,
                              kernel_initializer=kernel_initializer,
                              last_kernel_initializer=None,
                              output_dim=2 * 2 * n_intervals)
  weight_network = ValueNetwork((dataset.spec.observation,  # initial state
                                 dataset.spec.observation,  # cur state
                                 dataset.spec.action,       # cur action
                                 dataset.spec.observation), # next state
                                fc_layer_params=hidden_dims,
                                activation_fn=activation_fn,
                                kernel_initializer=kernel_initializer,
                                last_kernel_initializer=None,
                                output_dim=2 * n_intervals)
  
  nu_optimizer = tf.keras.optimizers.Adam(nu_learning_rate, beta_2=0.99)
  zeta_optimizer = tf.keras.optimizers.Adam(zeta_learning_rate, beta_2=0.99)
  weight_optimizer = tf.keras.optimizers.Adam(weight_learning_rate, beta_2=0.99)
  ################################################################################################################################################
  estimator = NeuralCoinDice(dataset.spec,
                             nu_network, zeta_network, weight_network,
                             nu_optimizer, zeta_optimizer, weight_optimizer,
                             gamma=gamma,
                             divergence_limit=divergence_limit,
                             f_exponent=f_exponent,
                             primal_form=primal_form,
                             nu_regularizer=nu_regularizer,
                             zeta_regularizer=zeta_regularizer,
                             algae_alpha=algae_alpha * np.array([1, 1]),
                             unbias_algae_alpha=False,
                             closed_form_weights=True,
                             num_samples=None)

  global_step = tf.Variable(0, dtype=tf.int64)
  tf.summary.experimental.set_step(global_step)
  ################################################################################################################################################

  @tf.function
  def one_step(env_steps, next_steps, initial_steps_batch):
    global_step.assign_add(1)
    with tf.summary.record_if(tf.math.mod(global_step, 25) == 0):
#       initial_steps_batch = tf.nest.map_structure(lambda t: t[:, 0, ...],
#                                                   initial_steps_batch)
      losses, _ = estimator.train_step(initial_steps_batch, env_steps, next_steps,
                                       target_policy)
    return losses
  ################################################################################################
  summary_writer = tf.summary.create_noop_writer()
  with summary_writer.as_default():
    running_losses = []
    running_estimates = []
    for step in range(num_steps):
      env_steps, next_steps = dataset.sample_step_and_next(batch_size)
      #transitions_batch = dataset.get_step(batch_size, num_steps=2)
      initial_steps_batch = dataset.sample_init_steps(batch_size)
#       initial_steps_batch, _ = dataset.get_episode(
#           batch_size, truncate_episode_at=1)
      losses = one_step(env_steps, next_steps, initial_steps_batch)
    
      running_losses.append([t.numpy() for t in losses])

      if step % 500 == 0 or step == num_steps - 1:
        print('step', step, 'losses', np.mean(running_losses, 0))
        estimate = np.mean(running_losses, 0)[0]
        for idx, est in enumerate(estimate):
          tf.summary.scalar('estimate%d' % idx, est)
        running_estimates.append(estimate)
        print('estimated confidence interval %s' % estimate)
        print('avg last 3 estimated confidence interval %s' %
              np.mean(running_estimates[-3:], axis=0))
        running_losses = []
  ################################################################################################
  if save_dir is not None:
    results_filename = os.path.join(save_dir, 'results.npy')
    with tf.io.gfile.GFile(results_filename, 'w') as f:
      np.save(f, running_estimates)
  print('Done!')


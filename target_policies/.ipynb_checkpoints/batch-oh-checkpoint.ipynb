{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch-wise training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !for i in $(sudo lsof /dev/nvidia2 | grep python | awk '{print $2}' | sort -u); do sudo kill -9 $i; done\n",
    "gpu_number = 7\n",
    "sd_G = 3\n",
    "###\n",
    "as_def = False # FQE. which option\n",
    "###\n",
    "N = 20\n",
    "T = 1000\n",
    "init_Q_ratio = 0.7\n",
    "incomplete_ratio = 5\n",
    "###\n",
    "FQE_paras = {\"lr\" : 1e-4, \"decay_steps\" : 1000, \"batch_size\" : 512, \"init_Q_ratio\" : init_Q_ratio\n",
    "             , \"as_def\" : as_def\n",
    "     , \"max_epoch\" : 10000, \"nn_verbose\" : 0, \"es_patience\" : 3, \"max_iter\" : 200, \"eps\" : 0.001}\n",
    "\n",
    "FQI_paras = {\"lr\" : 1e-4, \"decay_steps\" : int(1e3), \"batch_size\" : 128, \"max_epoch\" : 100, \"hiddens\" : [256, 256]\n",
    "             , \"es_patience\" : 2, \"max_iter\" : 1000, \"eps\" : 0.001, \"num_actions\" : 5}\n",
    "###\n",
    "gamma = .8\n",
    "rep = 20\n",
    "old_SA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mTrue Value: mean = -11.86 with std = 0.09\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "### estimate the target policy [fixed number of iterations?]\n",
    "curr_time = now()\n",
    "\n",
    "FQI_setting = \"N\" + str(N) + \"_\" + \"T\" + str(T) + \"_\" + \"sd\" + str(sd_G) + \"_\" + \"gamma\" + str(int(gamma * 100)) + \"_\" + \"lag\" + str(lag) + \"_\" + \"noiseless\" + str(noiseless)+ \"_\" + \"equal_A\" + str(equal_A)\n",
    "print(\"FQI_setting:\", FQI_setting)\n",
    "############################ Obtain the target policy (FQI) ############################\n",
    "pi_fqi = FQI.FQI(gpu_number = gpu_number, gamma = gamma, **FQI_paras)\n",
    "FQI_path = 'target_policies/' + FQI_setting\n",
    "\n",
    "seed = 0\n",
    "\"\"\" the behav policy here is so weird; I should train it in an online fashion. That is true. \"\"\"\n",
    "trajs_train_target = Ohio.OhioSimulator(sd_G = sd_G, T = T, N = N, behav = None\n",
    "                                       , lag = lag, noiseless = noiseless).simu_one_seed(seed)\n",
    "pi_fqi.train(trajs_train_target, verbose = 1, train_freq = 20, nn_verbose = 0)\n",
    "pi_fqi.model.save_weights(FQI_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ Obtain TRUE V of the target policy ############################\n",
    "V_true = Ohio.OhioSimulator(sd_G = sd_G, T = T, N = 10000, behav = pi_fqi\n",
    "                           , lag = lag, noiseless = noiseless).eval_policy(pi = pi_fqi, gamma = gamma)\n",
    "init_S = Ohio.OhioSimulator(sd_G = sd_G, T = 6, N = 1000, behav = pi_fqi\n",
    "                           , lag = lag, noiseless = noiseless).simu_init_S(seed)\n",
    "# hist(pi_fqi.get_A(init_S)) \n",
    "# hist([trajs_train[0][t][1] for t in range(990)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FQI_setting = \"N\" + str(N) + \"_\" + \"T\" + str(T) + \"_\" + \"sd\" + str(sd_G) + \"_\" + \"gamma\" + str(int(gamma * 100)) + \"_\" + \"lag\" + str(lag) + \"_\" + \"noiseless\" + str(noiseless)+ \"_\" + \"equal_A\" + str(equal_A)\n",
    "print(\"FQI_setting:\", FQI_setting)\n",
    "############################ Obtain the target policy (FQI) ############################\n",
    "pi_fqi = FQI.FQI(gpu_number = gpu_number, gamma = gamma, **FQI_paras)\n",
    "FQI_path = 'target_policies/' + FQI_setting\n",
    "seed = 0\n",
    "try:\n",
    "    pi_fqi.model.load_weights(FQI_path)\n",
    "except:\n",
    "    \n",
    "    \"\"\" the behav policy here is so weird; I should train it in an online fashion. That is true. \"\"\"\n",
    "    trajs_train_target = Ohio.OhioSimulator(sd_G = sd_G, T = T, N = N, behav = None\n",
    "                                           , lag = lag, noiseless = noiseless).simu_one_seed(seed)\n",
    "    pi_fqi.train(trajs_train_target, verbose = 1, train_freq = 20, nn_verbose = 0)\n",
    "    pi_fqi.model.save_weights(FQI_path)\n",
    "print(\"time cost for FQI = {:.1f} minutes\".format((now() - curr_time) / 60))\n",
    "############################ Obtain TRUE V of the target policy ############################\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
